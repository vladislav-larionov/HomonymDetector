/home/vladislav/projects/python/HomonymDetector/venv/bin/python /home/vladislav/projects/python/HomonymDetector/comp_sample_and_meaning/compare_sample_and_meaning.py 
Method w2v_emb
narusco_ru.json
metric = euclidean
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 89/520 0.1712
Total used words: 31/31

metric = euclidean
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 82/520 0.1577
Total used words: 31/31

metric = euclidean
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 124/520 0.2385
Total used words: 31/31

metric = euclidean
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 205/520 0.3942
Total used words: 31/31

metric = manhattan
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 91/520 0.1750
Total used words: 31/31

metric = manhattan
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 111/520 0.2135
Total used words: 31/31

metric = manhattan
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 121/520 0.2327
Total used words: 31/31

metric = manhattan
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 201/520 0.3865
Total used words: 31/31

metric = minkowski
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 89/520 0.1712
Total used words: 31/31

metric = minkowski
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 109/520 0.2096
Total used words: 31/31

metric = minkowski
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 124/520 0.2385
Total used words: 31/31

metric = minkowski
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 206/520 0.3962
Total used words: 31/31

metric = hamming
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 123/520 0.2365
Total used words: 31/31

metric = hamming
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 123/520 0.2365
Total used words: 31/31

metric = hamming
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 123/520 0.2365
Total used words: 31/31

metric = hamming
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 123/520 0.2365
Total used words: 31/31

metric = canberra
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 85/520 0.1635
Total used words: 31/31

metric = canberra
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 126/520 0.2423
Total used words: 31/31

metric = canberra
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 205/520 0.3942
Total used words: 31/31

metric = canberra
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 177/520 0.3404
Total used words: 31/31

metric = braycurtis
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 89/520 0.1712
Total used words: 31/31

metric = braycurtis
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 80/520 0.1538
Total used words: 31/31

metric = braycurtis
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 218/520 0.4192
Total used words: 31/31

metric = braycurtis
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 189/520 0.3635
Total used words: 31/31

________________________________________
navec_score
narusco_ru.json
use_lemma = False
remove_stop_words = False
Total: 135/520 0.2596
Total used words: 31/31

use_lemma = True
remove_stop_words = False
Total: 134/520 0.2577
Total used words: 31/31

use_lemma = False
remove_stop_words = True
Total: 95/520 0.1827
Total used words: 31/31

use_lemma = True
remove_stop_words = True
Total: 97/520 0.1865
Total used words: 31/31

________________________________________
gensim_pretrainde
narusco_ru.json
Model: word2vec-ruscorpora-300
metric = euclidean
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 106/520 0.2038
Total used words: 31/31

metric = euclidean
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 195/520 0.3750
Total used words: 31/31

metric = euclidean
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 106/520 0.2038
Total used words: 31/31

metric = euclidean
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 196/520 0.3769
Total used words: 31/31

metric = manhattan
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 108/520 0.2077
Total used words: 31/31

metric = manhattan
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 195/520 0.3750
Total used words: 31/31

metric = manhattan
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 108/520 0.2077
Total used words: 31/31

metric = manhattan
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 195/520 0.3750
Total used words: 31/31

metric = minkowski
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 106/520 0.2038
Total used words: 31/31

metric = minkowski
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 195/520 0.3750
Total used words: 31/31

metric = minkowski
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 106/520 0.2038
Total used words: 31/31

metric = minkowski
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 196/520 0.3769
Total used words: 31/31

metric = hamming
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 122/520 0.2346
Total used words: 31/31

metric = hamming
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 123/520 0.2365
Total used words: 31/31

metric = hamming
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 122/520 0.2346
Total used words: 31/31

metric = hamming
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 123/520 0.2365
Total used words: 31/31

metric = canberra
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 117/520 0.2250
Total used words: 31/31

metric = canberra
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 135/520 0.2596
Total used words: 31/31

metric = canberra
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 117/520 0.2250
Total used words: 31/31

metric = canberra
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 136/520 0.2615
Total used words: 31/31

metric = braycurtis
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 117/520 0.2250
Total used words: 31/31

metric = braycurtis
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 121/520 0.2327
Total used words: 31/31

metric = braycurtis
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 117/520 0.2250
Total used words: 31/31

metric = braycurtis
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 121/520 0.2327
Total used words: 31/31

________________________________________
d2v_emb
narusco_ru.json
metric = euclidean
use_lemma = False
remove_stop_words = False
Total: 163/520 0.3135
Total used words: 31/31

metric = euclidean
use_lemma = True
remove_stop_words = False
Total: 215/520 0.4135
Total used words: 31/31

metric = euclidean
use_lemma = False
remove_stop_words = True
Total: 176/520 0.3385
Total used words: 31/31

metric = euclidean
use_lemma = True
remove_stop_words = True
Total: 174/520 0.3346
Total used words: 31/31

metric = manhattan
use_lemma = False
remove_stop_words = False
Total: 218/520 0.4192
Total used words: 31/31

metric = manhattan
use_lemma = True
remove_stop_words = False
Total: 258/520 0.4962
Total used words: 31/31

metric = manhattan
use_lemma = False
remove_stop_words = True
Total: 176/520 0.3385
Total used words: 31/31

metric = manhattan
use_lemma = True
remove_stop_words = True
Total: 170/520 0.3269
Total used words: 31/31

metric = minkowski
use_lemma = False
remove_stop_words = False
Total: 245/520 0.4712
Total used words: 31/31

metric = minkowski
use_lemma = True
remove_stop_words = False
Total: 226/520 0.4346
Total used words: 31/31

metric = minkowski
use_lemma = False
remove_stop_words = True
Total: 174/520 0.3346
Total used words: 31/31

metric = minkowski
use_lemma = True
remove_stop_words = True
Total: 170/520 0.3269
Total used words: 31/31

metric = hamming
use_lemma = False
remove_stop_words = False
Total: 123/520 0.2365
Total used words: 31/31

metric = hamming
use_lemma = True
remove_stop_words = False
Total: 123/520 0.2365
Total used words: 31/31

metric = hamming
use_lemma = False
remove_stop_words = True
Total: 123/520 0.2365
Total used words: 31/31

metric = hamming
use_lemma = True
remove_stop_words = True
Total: 123/520 0.2365
Total used words: 31/31

metric = canberra
use_lemma = False
remove_stop_words = False
Total: 150/520 0.2885
Total used words: 31/31

metric = canberra
use_lemma = True
remove_stop_words = False
Total: 262/520 0.5038
Total used words: 31/31

metric = canberra
use_lemma = False
remove_stop_words = True
Total: 175/520 0.3365
Total used words: 31/31

metric = canberra
use_lemma = True
remove_stop_words = True
Total: 157/520 0.3019
Total used words: 31/31

metric = braycurtis
use_lemma = False
remove_stop_words = False
Total: 196/520 0.3769
Total used words: 31/31

metric = braycurtis
use_lemma = True
remove_stop_words = False
Total: 268/520 0.5154
Total used words: 31/31

metric = braycurtis
use_lemma = False
remove_stop_words = True
Total: 158/520 0.3038
Total used words: 31/31

metric = braycurtis
use_lemma = True
remove_stop_words = True
Total: 162/520 0.3115
Total used words: 31/31

________________________________________
bert_score
narusco_ru.json
model cointegrated/rubert-tiny
metric euclidean
Total: 213/520 0.4096
Total used words: 31/31

model cointegrated/rubert-tiny
metric manhattan
Total: 235/520 0.4519
Total used words: 31/31

model cointegrated/rubert-tiny
metric minkowski
Total: 213/520 0.4096
Total used words: 31/31

model cointegrated/rubert-tiny
metric hamming
Total: 123/520 0.2365
Total used words: 31/31

model cointegrated/rubert-tiny
metric canberra
Total: 217/520 0.4173
Total used words: 31/31

model cointegrated/rubert-tiny
metric braycurtis
Total: 235/520 0.4519
Total used words: 31/31

model cointegrated/rubert-tiny2
metric euclidean
Total: 250/520 0.4808
Total used words: 31/31

model cointegrated/rubert-tiny2
metric manhattan
Total: 261/520 0.5019
Total used words: 31/31

model cointegrated/rubert-tiny2
metric minkowski
Total: 250/520 0.4808
Total used words: 31/31

model cointegrated/rubert-tiny2
metric hamming
Total: 123/520 0.2365
Total used words: 31/31

model cointegrated/rubert-tiny2
metric canberra
Total: 253/520 0.4865
Total used words: 31/31

model cointegrated/rubert-tiny2
metric braycurtis
Total: 251/520 0.4827
Total used words: 31/31

model sberbank-ai/sbert_large_nlu_ru
metric euclidean
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 255/520 0.4904
Total used words: 31/31

model sberbank-ai/sbert_large_nlu_ru
metric manhattan
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 249/520 0.4788
Total used words: 31/31

model sberbank-ai/sbert_large_nlu_ru
metric minkowski
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 255/520 0.4904
Total used words: 31/31

model sberbank-ai/sbert_large_nlu_ru
metric hamming
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 123/520 0.2365
Total used words: 31/31

model sberbank-ai/sbert_large_nlu_ru
metric canberra
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 239/520 0.4596
Total used words: 31/31

model sberbank-ai/sbert_large_nlu_ru
metric braycurtis
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 250/520 0.4808
Total used words: 31/31

model DeepPavlov/rubert-base-cased-sentence
metric euclidean
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 231/520 0.4442
Total used words: 31/31

model DeepPavlov/rubert-base-cased-sentence
metric manhattan
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 228/520 0.4385
Total used words: 31/31

model DeepPavlov/rubert-base-cased-sentence
metric minkowski
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 231/520 0.4442
Total used words: 31/31

model DeepPavlov/rubert-base-cased-sentence
metric hamming
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 123/520 0.2365
Total used words: 31/31

model DeepPavlov/rubert-base-cased-sentence
metric canberra
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 225/520 0.4327
Total used words: 31/31

model DeepPavlov/rubert-base-cased-sentence
metric braycurtis
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 226/520 0.4346
Total used words: 31/31

model DeepPavlov/rubert-base-cased
metric euclidean
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 215/520 0.4135
Total used words: 31/31

model DeepPavlov/rubert-base-cased
metric manhattan
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 210/520 0.4038
Total used words: 31/31

model DeepPavlov/rubert-base-cased
metric minkowski
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 215/520 0.4135
Total used words: 31/31

model DeepPavlov/rubert-base-cased
metric hamming
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 123/520 0.2365
Total used words: 31/31

model DeepPavlov/rubert-base-cased
metric canberra
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 241/520 0.4635
Total used words: 31/31

model DeepPavlov/rubert-base-cased
metric braycurtis
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 225/520 0.4327
Total used words: 31/31

model inkoziev/sbert_synonymy
metric euclidean
Total: 194/520 0.3731
Total used words: 31/31

model inkoziev/sbert_synonymy
metric manhattan
Total: 190/520 0.3654
Total used words: 31/31

model inkoziev/sbert_synonymy
metric minkowski
Total: 194/520 0.3731
Total used words: 31/31

model inkoziev/sbert_synonymy
metric hamming
Total: 123/520 0.2365
Total used words: 31/31

model inkoziev/sbert_synonymy
metric canberra
Total: 177/520 0.3404
Total used words: 31/31

model inkoziev/sbert_synonymy
metric braycurtis
Total: 183/520 0.3519
Total used words: 31/31


Process finished with exit code 0

