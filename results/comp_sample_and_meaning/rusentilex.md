/home/vladislav/projects/python/HomonymDetector/venv/bin/python /home/vladislav/projects/python/HomonymDetector/comp_sample_and_meaning/compare_sample_and_meaning.py 
Method w2v_emb
rusentilex.json
metric = euclidean
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = euclidean
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = euclidean
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = euclidean
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 23/116 0.1983
Total used words: 8/8

metric = manhattan
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = manhattan
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 37/116 0.3190
Total used words: 8/8

metric = manhattan
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 40/116 0.3448
Total used words: 8/8

metric = manhattan
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 22/116 0.1897
Total used words: 8/8

metric = minkowski
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = minkowski
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = minkowski
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = minkowski
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 23/116 0.1983
Total used words: 8/8

metric = hamming
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 61/116 0.5259
Total used words: 8/8

metric = hamming
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 61/116 0.5259
Total used words: 8/8

metric = hamming
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 61/116 0.5259
Total used words: 8/8

metric = hamming
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 61/116 0.5259
Total used words: 8/8

metric = canberra
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 39/116 0.3362
Total used words: 8/8

metric = canberra
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 39/116 0.3362
Total used words: 8/8

metric = canberra
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 34/116 0.2931
Total used words: 8/8

metric = canberra
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 34/116 0.2931
Total used words: 8/8

metric = braycurtis
use_lemma = False
remove_stop_words = False
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

metric = braycurtis
use_lemma = True
remove_stop_words = False
get_mean_vector
Total: 38/116 0.3276
Total used words: 8/8

metric = braycurtis
use_lemma = False
remove_stop_words = True
get_mean_vector
Total: 34/116 0.2931
Total used words: 8/8

metric = braycurtis
use_lemma = True
remove_stop_words = True
get_mean_vector
Total: 41/116 0.3534
Total used words: 8/8

________________________________________
navec_score
rusentilex.json
use_lemma = False
remove_stop_words = False
Total: 25/116 0.2155
Total used words: 8/8

use_lemma = True
remove_stop_words = False
Total: 24/116 0.2069
Total used words: 8/8

use_lemma = False
remove_stop_words = True
Total: 23/116 0.1983
Total used words: 8/8

use_lemma = True
remove_stop_words = True
Total: 21/116 0.1810
Total used words: 8/8

________________________________________
gensim_pretrainde
rusentilex.json
Model: word2vec-ruscorpora-300
metric = euclidean
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 44/116 0.3793
Total used words: 8/8

metric = euclidean
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 58/116 0.5000
Total used words: 8/8

metric = euclidean
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 44/116 0.3793
Total used words: 8/8

metric = euclidean
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 59/116 0.5086
Total used words: 8/8

metric = manhattan
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 44/116 0.3793
Total used words: 8/8

metric = manhattan
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 58/116 0.5000
Total used words: 8/8

metric = manhattan
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 44/116 0.3793
Total used words: 8/8

metric = manhattan
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 58/116 0.5000
Total used words: 8/8

metric = minkowski
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 44/116 0.3793
Total used words: 8/8

metric = minkowski
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 58/116 0.5000
Total used words: 8/8

metric = minkowski
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 44/116 0.3793
Total used words: 8/8

metric = minkowski
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 59/116 0.5086
Total used words: 8/8

metric = hamming
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 60/116 0.5172
Total used words: 8/8

metric = hamming
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 61/116 0.5259
Total used words: 8/8

metric = hamming
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 60/116 0.5172
Total used words: 8/8

metric = hamming
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 61/116 0.5259
Total used words: 8/8

metric = canberra
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 37/116 0.3190
Total used words: 8/8

metric = canberra
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 37/116 0.3190
Total used words: 8/8

metric = canberra
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 37/116 0.3190
Total used words: 8/8

metric = canberra
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 38/116 0.3276
Total used words: 8/8

metric = braycurtis
use_lemma = False
remove_stop_words = False
sum_vectors
Total: 38/116 0.3276
Total used words: 8/8

metric = braycurtis
use_lemma = True
remove_stop_words = False
sum_vectors
Total: 36/116 0.3103
Total used words: 8/8

metric = braycurtis
use_lemma = False
remove_stop_words = True
sum_vectors
Total: 38/116 0.3276
Total used words: 8/8

metric = braycurtis
use_lemma = True
remove_stop_words = True
sum_vectors
Total: 36/116 0.3103
Total used words: 8/8

________________________________________
d2v_emb
rusentilex.json
metric = euclidean
use_lemma = False
remove_stop_words = False
Total: 45/116 0.3879
Total used words: 8/8

metric = euclidean
use_lemma = True
remove_stop_words = False
Total: 53/116 0.4569
Total used words: 8/8

metric = euclidean
use_lemma = False
remove_stop_words = True
Total: 38/116 0.3276
Total used words: 8/8

metric = euclidean
use_lemma = True
remove_stop_words = True
Total: 38/116 0.3276
Total used words: 8/8

metric = manhattan
use_lemma = False
remove_stop_words = False
Total: 45/116 0.3879
Total used words: 8/8

metric = manhattan
use_lemma = True
remove_stop_words = False
Total: 53/116 0.4569
Total used words: 8/8

metric = manhattan
use_lemma = False
remove_stop_words = True
Total: 39/116 0.3362
Total used words: 8/8

metric = manhattan
use_lemma = True
remove_stop_words = True
Total: 36/116 0.3103
Total used words: 8/8

metric = minkowski
use_lemma = False
remove_stop_words = False
Total: 45/116 0.3879
Total used words: 8/8

metric = minkowski
use_lemma = True
remove_stop_words = False
Total: 53/116 0.4569
Total used words: 8/8

metric = minkowski
use_lemma = False
remove_stop_words = True
Total: 38/116 0.3276
Total used words: 8/8

metric = minkowski
use_lemma = True
remove_stop_words = True
Total: 38/116 0.3276
Total used words: 8/8

metric = hamming
use_lemma = False
remove_stop_words = False
Total: 61/116 0.5259
Total used words: 8/8

metric = hamming
use_lemma = True
remove_stop_words = False
Total: 61/116 0.5259
Total used words: 8/8

metric = hamming
use_lemma = False
remove_stop_words = True
Total: 61/116 0.5259
Total used words: 8/8

metric = hamming
use_lemma = True
remove_stop_words = True
Total: 61/116 0.5259
Total used words: 8/8

metric = canberra
use_lemma = False
remove_stop_words = False
Total: 44/116 0.3793
Total used words: 8/8

metric = canberra
use_lemma = True
remove_stop_words = False
Total: 41/116 0.3534
Total used words: 8/8

metric = canberra
use_lemma = False
remove_stop_words = True
Total: 38/116 0.3276
Total used words: 8/8

metric = canberra
use_lemma = True
remove_stop_words = True
Total: 39/116 0.3362
Total used words: 8/8

metric = braycurtis
use_lemma = False
remove_stop_words = False
Total: 45/116 0.3879
Total used words: 8/8

metric = braycurtis
use_lemma = True
remove_stop_words = False
Total: 52/116 0.4483
Total used words: 8/8

metric = braycurtis
use_lemma = False
remove_stop_words = True
Total: 38/116 0.3276
Total used words: 8/8

metric = braycurtis
use_lemma = True
remove_stop_words = True
Total: 38/116 0.3276
Total used words: 8/8

________________________________________
bert_score
rusentilex.json
model cointegrated/rubert-tiny
metric euclidean
Total: 25/116 0.2155
Total used words: 8/8

model cointegrated/rubert-tiny
metric manhattan
Total: 27/116 0.2328
Total used words: 8/8

model cointegrated/rubert-tiny
metric minkowski
Total: 25/116 0.2155
Total used words: 8/8

model cointegrated/rubert-tiny
metric hamming
Total: 61/116 0.5259
Total used words: 8/8

model cointegrated/rubert-tiny
metric canberra
Total: 26/116 0.2241
Total used words: 8/8

model cointegrated/rubert-tiny
metric braycurtis
Total: 27/116 0.2328
Total used words: 8/8

model cointegrated/rubert-tiny2
metric euclidean
Total: 43/116 0.3707
Total used words: 8/8

model cointegrated/rubert-tiny2
metric manhattan
Total: 48/116 0.4138
Total used words: 8/8

model cointegrated/rubert-tiny2
metric minkowski
Total: 43/116 0.3707
Total used words: 8/8

model cointegrated/rubert-tiny2
metric hamming
Total: 61/116 0.5259
Total used words: 8/8

model cointegrated/rubert-tiny2
metric canberra
Total: 42/116 0.3621
Total used words: 8/8

model cointegrated/rubert-tiny2
metric braycurtis
Total: 41/116 0.3534
Total used words: 8/8

model sberbank-ai/sbert_large_nlu_ru
metric euclidean
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 40/116 0.3448
Total used words: 8/8

model sberbank-ai/sbert_large_nlu_ru
metric manhattan
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 40/116 0.3448
Total used words: 8/8

model sberbank-ai/sbert_large_nlu_ru
metric minkowski
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 40/116 0.3448
Total used words: 8/8

model sberbank-ai/sbert_large_nlu_ru
metric hamming
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 61/116 0.5259
Total used words: 8/8

model sberbank-ai/sbert_large_nlu_ru
metric canberra
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 37/116 0.3190
Total used words: 8/8

model sberbank-ai/sbert_large_nlu_ru
metric braycurtis
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 40/116 0.3448
Total used words: 8/8

model DeepPavlov/rubert-base-cased-sentence
metric euclidean
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 40/116 0.3448
Total used words: 8/8

model DeepPavlov/rubert-base-cased-sentence
metric manhattan
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 35/116 0.3017
Total used words: 8/8

model DeepPavlov/rubert-base-cased-sentence
metric minkowski
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 40/116 0.3448
Total used words: 8/8

model DeepPavlov/rubert-base-cased-sentence
metric hamming
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 61/116 0.5259
Total used words: 8/8

model DeepPavlov/rubert-base-cased-sentence
metric canberra
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 34/116 0.2931
Total used words: 8/8

model DeepPavlov/rubert-base-cased-sentence
metric braycurtis
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 33/116 0.2845
Total used words: 8/8

model DeepPavlov/rubert-base-cased
metric euclidean
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 42/116 0.3621
Total used words: 8/8

model DeepPavlov/rubert-base-cased
metric manhattan
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 38/116 0.3276
Total used words: 8/8

model DeepPavlov/rubert-base-cased
metric minkowski
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 42/116 0.3621
Total used words: 8/8

model DeepPavlov/rubert-base-cased
metric hamming
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 61/116 0.5259
Total used words: 8/8

model DeepPavlov/rubert-base-cased
metric canberra
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 49/116 0.4224
Total used words: 8/8

model DeepPavlov/rubert-base-cased
metric braycurtis
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 42/116 0.3621
Total used words: 8/8

model inkoziev/sbert_synonymy
metric euclidean
Total: 45/116 0.3879
Total used words: 8/8

model inkoziev/sbert_synonymy
metric manhattan
Total: 46/116 0.3966
Total used words: 8/8

model inkoziev/sbert_synonymy
metric minkowski
Total: 45/116 0.3879
Total used words: 8/8

model inkoziev/sbert_synonymy
metric hamming
Total: 61/116 0.5259
Total used words: 8/8

model inkoziev/sbert_synonymy
metric canberra
Total: 40/116 0.3448
Total used words: 8/8

model inkoziev/sbert_synonymy
metric braycurtis
Total: 43/116 0.3707
Total used words: 8/8


Process finished with exit code 0

