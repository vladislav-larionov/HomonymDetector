/home/vladislav/projects/python/HomonymDetector/venv/bin/python /home/vladislav/projects/python/HomonymDetector/comp_sample_and_meaning/compare_sample_and_meaning.py 
Method w2v_emb
homonyms_ru.json
metric = euclidean
use_lemma = False
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 146/419 0.3484
Total used words: 57/57

metric = euclidean
use_lemma = True
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 134/419 0.3198
Total used words: 57/57

metric = euclidean
use_lemma = False
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 155/419 0.3699
Total used words: 57/57

metric = euclidean
use_lemma = True
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 139/419 0.3317
Total used words: 57/57

metric = manhattan
use_lemma = False
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 144/419 0.3437
Total used words: 57/57

metric = manhattan
use_lemma = True
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 132/419 0.3150
Total used words: 57/57

metric = manhattan
use_lemma = False
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 153/419 0.3652
Total used words: 57/57

metric = manhattan
use_lemma = True
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 141/419 0.3365
Total used words: 57/57

metric = minkowski
use_lemma = False
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 146/419 0.3484
Total used words: 57/57

metric = minkowski
use_lemma = True
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 134/419 0.3198
Total used words: 57/57

metric = minkowski
use_lemma = False
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 155/419 0.3699
Total used words: 57/57

metric = minkowski
use_lemma = True
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 139/419 0.3317
Total used words: 57/57

metric = hamming
use_lemma = False
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 151/419 0.3604
Total used words: 57/57

metric = hamming
use_lemma = True
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 151/419 0.3604
Total used words: 57/57

metric = hamming
use_lemma = False
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 151/419 0.3604
Total used words: 57/57

metric = hamming
use_lemma = True
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 151/419 0.3604
Total used words: 57/57

metric = canberra
use_lemma = False
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 141/419 0.3365
Total used words: 57/57

metric = canberra
use_lemma = True
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 140/419 0.3341
Total used words: 57/57

metric = canberra
use_lemma = False
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 143/419 0.3413
Total used words: 57/57

metric = canberra
use_lemma = True
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 146/419 0.3484
Total used words: 57/57

metric = braycurtis
use_lemma = False
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 136/419 0.3246
Total used words: 57/57

metric = braycurtis
use_lemma = True
remove_stop_words = False
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 131/419 0.3126
Total used words: 57/57

metric = braycurtis
use_lemma = False
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 140/419 0.3341
Total used words: 57/57

metric = braycurtis
use_lemma = True
remove_stop_words = True
get_mean_vector
words_to_vectors []
cannot compute mean with no input
Total: 148/419 0.3532
Total used words: 57/57

________________________________________
navec_score
homonyms_ru.json
use_lemma = False
remove_stop_words = False
Total: 90/419 0.2148
Total used words: 57/57

use_lemma = True
remove_stop_words = False
Total: 91/419 0.2172
Total used words: 57/57

use_lemma = False
remove_stop_words = True
Total: 62/419 0.1480
Total used words: 57/57

use_lemma = True
remove_stop_words = True
Total: 71/419 0.1695
Total used words: 57/57

________________________________________
gensim_pretrainde
homonyms_ru.json
Model: word2vec-ruscorpora-300
metric = euclidean
use_lemma = False
remove_stop_words = False
sum_vectors
[]
Total: 157/419 0.3747
Total used words: 57/57

metric = euclidean
use_lemma = True
remove_stop_words = False
sum_vectors
[]
Total: 180/419 0.4296
Total used words: 57/57

metric = euclidean
use_lemma = False
remove_stop_words = True
sum_vectors
[]
Total: 157/419 0.3747
Total used words: 57/57

metric = euclidean
use_lemma = True
remove_stop_words = True
sum_vectors
[]
Total: 180/419 0.4296
Total used words: 57/57

metric = manhattan
use_lemma = False
remove_stop_words = False
sum_vectors
[]
Total: 157/419 0.3747
Total used words: 57/57

metric = manhattan
use_lemma = True
remove_stop_words = False
sum_vectors
[]
Total: 181/419 0.4320
Total used words: 57/57

metric = manhattan
use_lemma = False
remove_stop_words = True
sum_vectors
[]
Total: 157/419 0.3747
Total used words: 57/57

metric = manhattan
use_lemma = True
remove_stop_words = True
sum_vectors
[]
Total: 181/419 0.4320
Total used words: 57/57

metric = minkowski
use_lemma = False
remove_stop_words = False
sum_vectors
[]
Total: 157/419 0.3747
Total used words: 57/57

metric = minkowski
use_lemma = True
remove_stop_words = False
sum_vectors
[]
Total: 180/419 0.4296
Total used words: 57/57

metric = minkowski
use_lemma = False
remove_stop_words = True
sum_vectors
[]
Total: 157/419 0.3747
Total used words: 57/57

metric = minkowski
use_lemma = True
remove_stop_words = True
sum_vectors
[]
Total: 180/419 0.4296
Total used words: 57/57

metric = hamming
use_lemma = False
remove_stop_words = False
sum_vectors
[]
Total: 145/419 0.3461
Total used words: 57/57

metric = hamming
use_lemma = True
remove_stop_words = False
sum_vectors
[]
Total: 151/419 0.3604
Total used words: 57/57

metric = hamming
use_lemma = False
remove_stop_words = True
sum_vectors
[]
Total: 145/419 0.3461
Total used words: 57/57

metric = hamming
use_lemma = True
remove_stop_words = True
sum_vectors
[]
Total: 151/419 0.3604
Total used words: 57/57

metric = canberra
use_lemma = False
remove_stop_words = False
sum_vectors
[]
Total: 155/419 0.3699
Total used words: 57/57

metric = canberra
use_lemma = True
remove_stop_words = False
sum_vectors
[]
Total: 216/419 0.5155
Total used words: 57/57

metric = canberra
use_lemma = False
remove_stop_words = True
sum_vectors
[]
Total: 155/419 0.3699
Total used words: 57/57

metric = canberra
use_lemma = True
remove_stop_words = True
sum_vectors
[]
Total: 216/419 0.5155
Total used words: 57/57

metric = braycurtis
use_lemma = False
remove_stop_words = False
sum_vectors
[]
Total: 164/419 0.3914
Total used words: 57/57

metric = braycurtis
use_lemma = True
remove_stop_words = False
sum_vectors
[]
Total: 212/419 0.5060
Total used words: 57/57

metric = braycurtis
use_lemma = False
remove_stop_words = True
sum_vectors
[]
Total: 164/419 0.3914
Total used words: 57/57

metric = braycurtis
use_lemma = True
remove_stop_words = True
sum_vectors
[]
Total: 212/419 0.5060
Total used words: 57/57

________________________________________
d2v_emb
homonyms_ru.json
metric = euclidean
use_lemma = False
remove_stop_words = False
Total: 136/420 0.3238
Total used words: 57/57

metric = euclidean
use_lemma = True
remove_stop_words = False
Total: 130/420 0.3095
Total used words: 57/57

metric = euclidean
use_lemma = False
remove_stop_words = True
Total: 134/420 0.3190
Total used words: 57/57

metric = euclidean
use_lemma = True
remove_stop_words = True
Total: 131/420 0.3119
Total used words: 57/57

metric = manhattan
use_lemma = False
remove_stop_words = False
Total: 133/420 0.3167
Total used words: 57/57

metric = manhattan
use_lemma = True
remove_stop_words = False
Total: 129/420 0.3071
Total used words: 57/57

metric = manhattan
use_lemma = False
remove_stop_words = True
Total: 124/420 0.2952
Total used words: 57/57

metric = manhattan
use_lemma = True
remove_stop_words = True
Total: 136/420 0.3238
Total used words: 57/57

metric = minkowski
use_lemma = False
remove_stop_words = False
Total: 136/420 0.3238
Total used words: 57/57

metric = minkowski
use_lemma = True
remove_stop_words = False
Total: 130/420 0.3095
Total used words: 57/57

metric = minkowski
use_lemma = False
remove_stop_words = True
Total: 133/420 0.3167
Total used words: 57/57

metric = minkowski
use_lemma = True
remove_stop_words = True
Total: 131/420 0.3119
Total used words: 57/57

metric = hamming
use_lemma = False
remove_stop_words = False
Total: 151/420 0.3595
Total used words: 57/57

metric = hamming
use_lemma = True
remove_stop_words = False
Total: 151/420 0.3595
Total used words: 57/57

metric = hamming
use_lemma = False
remove_stop_words = True
Total: 151/420 0.3595
Total used words: 57/57

metric = hamming
use_lemma = True
remove_stop_words = True
Total: 151/420 0.3595
Total used words: 57/57

metric = canberra
use_lemma = False
remove_stop_words = False
Total: 137/420 0.3262
Total used words: 57/57

metric = canberra
use_lemma = True
remove_stop_words = False
Total: 131/420 0.3119
Total used words: 57/57

metric = canberra
use_lemma = False
remove_stop_words = True
Total: 146/420 0.3476
Total used words: 57/57

metric = canberra
use_lemma = True
remove_stop_words = True
Total: 136/420 0.3238
Total used words: 57/57

metric = braycurtis
use_lemma = False
remove_stop_words = False
Total: 133/420 0.3167
Total used words: 57/57

metric = braycurtis
use_lemma = True
remove_stop_words = False
Total: 129/420 0.3071
Total used words: 57/57

metric = braycurtis
use_lemma = False
remove_stop_words = True
Total: 141/420 0.3357
Total used words: 57/57

metric = braycurtis
use_lemma = True
remove_stop_words = True
Total: 142/420 0.3381
Total used words: 57/57

________________________________________
bert_score
homonyms_ru.json
model cointegrated/rubert-tiny
metric euclidean
Total: 185/420 0.4405
Total used words: 57/57

model cointegrated/rubert-tiny
metric manhattan
Total: 183/420 0.4357
Total used words: 57/57

model cointegrated/rubert-tiny
metric minkowski
Total: 185/420 0.4405
Total used words: 57/57

model cointegrated/rubert-tiny
metric hamming
Total: 151/420 0.3595
Total used words: 57/57

model cointegrated/rubert-tiny
metric canberra
Total: 193/420 0.4595
Total used words: 57/57

model cointegrated/rubert-tiny
metric braycurtis
Total: 186/420 0.4429
Total used words: 57/57

model cointegrated/rubert-tiny2
metric euclidean
Total: 219/420 0.5214
Total used words: 57/57

model cointegrated/rubert-tiny2
metric manhattan
Total: 230/420 0.5476
Total used words: 57/57

model cointegrated/rubert-tiny2
metric minkowski
Total: 219/420 0.5214
Total used words: 57/57

model cointegrated/rubert-tiny2
metric hamming
Total: 151/420 0.3595
Total used words: 57/57

model cointegrated/rubert-tiny2
metric canberra
Total: 213/420 0.5071
Total used words: 57/57

model cointegrated/rubert-tiny2
metric braycurtis
Total: 222/420 0.5286
Total used words: 57/57

model sberbank-ai/sbert_large_nlu_ru
metric euclidean
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 207/420 0.4929
Total used words: 57/57

model sberbank-ai/sbert_large_nlu_ru
metric manhattan
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 208/420 0.4952
Total used words: 57/57

model sberbank-ai/sbert_large_nlu_ru
metric minkowski
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 207/420 0.4929
Total used words: 57/57

model sberbank-ai/sbert_large_nlu_ru
metric hamming
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 151/420 0.3595
Total used words: 57/57

model sberbank-ai/sbert_large_nlu_ru
metric canberra
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 202/420 0.4810
Total used words: 57/57

model sberbank-ai/sbert_large_nlu_ru
metric braycurtis
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 205/420 0.4881
Total used words: 57/57

model DeepPavlov/rubert-base-cased-sentence
metric euclidean
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 227/420 0.5405
Total used words: 57/57

model DeepPavlov/rubert-base-cased-sentence
metric manhattan
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 225/420 0.5357
Total used words: 57/57

model DeepPavlov/rubert-base-cased-sentence
metric minkowski
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 227/420 0.5405
Total used words: 57/57

model DeepPavlov/rubert-base-cased-sentence
metric hamming
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 151/420 0.3595
Total used words: 57/57

model DeepPavlov/rubert-base-cased-sentence
metric canberra
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 221/420 0.5262
Total used words: 57/57

model DeepPavlov/rubert-base-cased-sentence
metric braycurtis
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 226/420 0.5381
Total used words: 57/57

model DeepPavlov/rubert-base-cased
metric euclidean
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 137/420 0.3262
Total used words: 57/57

model DeepPavlov/rubert-base-cased
metric manhattan
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 135/420 0.3214
Total used words: 57/57

model DeepPavlov/rubert-base-cased
metric minkowski
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 137/420 0.3262
Total used words: 57/57

model DeepPavlov/rubert-base-cased
metric hamming
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 151/420 0.3595
Total used words: 57/57

model DeepPavlov/rubert-base-cased
metric canberra
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 162/420 0.3857
Total used words: 57/57

model DeepPavlov/rubert-base-cased
metric braycurtis
Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Total: 148/420 0.3524
Total used words: 57/57

model inkoziev/sbert_synonymy
metric euclidean
Total: 174/420 0.4143
Total used words: 57/57

model inkoziev/sbert_synonymy
metric manhattan
Total: 175/420 0.4167
Total used words: 57/57

model inkoziev/sbert_synonymy
metric minkowski
Total: 174/420 0.4143
Total used words: 57/57

model inkoziev/sbert_synonymy
metric hamming
Total: 151/420 0.3595
Total used words: 57/57

model inkoziev/sbert_synonymy
metric canberra
Total: 171/420 0.4071
Total used words: 57/57

model inkoziev/sbert_synonymy
metric braycurtis
Total: 167/420 0.3976
Total used words: 57/57


Process finished with exit code 0

